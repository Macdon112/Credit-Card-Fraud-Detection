{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f8c924",
   "metadata": {},
   "source": [
    "### Credit Card fraud detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2def39d",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7fbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, roc_auc_score, RocCurveDisplay, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from category_encoders import WOEEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53728f75",
   "metadata": {},
   "source": [
    "#### Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52373e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"fraud test.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827919d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c138508",
   "metadata": {},
   "source": [
    "#### Checking null values in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c29d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0574b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5d28d",
   "metadata": {},
   "source": [
    "#### Distribution of fraudulent transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = df[df['is_fraud'] == 1] \n",
    "non_fraud = df[df['is_fraud'] == 0] \n",
    "outlierFraction = len(fraud)/float(len(non_fraud)) \n",
    "print(outlierFraction) \n",
    "print('Fraud Cases: {}'.format(len(df[df['is_fraud'] == 1]))) \n",
    "print('Non fraud Transactions: {}'.format(len(df[df['is_fraud'] == 0]))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e076aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_counts = df['is_fraud'].value_counts()\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.pie(fraud_counts, labels=['Non-fraud', 'Fraud'], autopct='%1.1f%%', colors=['green', 'yellow'])\n",
    "plt.title('Distribution of fraud and non-fraud')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610afa6c",
   "metadata": {},
   "source": [
    "#### Distribution of gender verses fraudulent transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0387361",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(8, 5))\n",
    "\n",
    "# Gender distribution Pie chart\n",
    "explode = [0.1, 0.1]\n",
    "df.groupby('gender')['is_fraud'].count().plot.pie(explode=explode, autopct=\"%1.1f%%\", ax=axs[0], colors=['skyblue', 'lightcoral'])\n",
    "axs[0].set_title(\"Gender Distribution\")\n",
    "\n",
    "# Fraud Status by gender Count Plot\n",
    "ax = sns.countplot(x=\"gender\", hue=\"is_fraud\", data=df, ax=axs[1], palette='Set2')\n",
    "\n",
    "# Add values on top of each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "# Set labels and title\n",
    "axs[1].set_title(\"Distribution of Gender with Fraud Status\")\n",
    "axs[1].set_xlabel(\"Gender\")\n",
    "axs[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4fb5b9",
   "metadata": {},
   "source": [
    "#### Category verses fraudulent and non-fraudulent activities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce852896",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = \"amt\", y = \"category\", data = df, hue = \"is_fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade9b9a-efc7-4e4c-9073-b61a410f41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(results_path, \"Category verses fraudulent and non-fraudulent activities\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2372dfa",
   "metadata": {},
   "source": [
    "#### Distribution of fraudulent activities by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'trans_date_trans_time' column to datetime format\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d43d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'trans_date_trans_time' column to datetime and extract the hour\n",
    "df['hour'] = pd.to_datetime(df['trans_date_trans_time']).dt.hour\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# Plot histogram for non-fraudulent transactions\n",
    "sns.histplot(x='hour', data=df[df[\"is_fraud\"] == 0],\n",
    "             stat=\"density\", bins=24, ax=axes[0], color=\"orange\")\n",
    "axes[0].set_title(\"Not Fraud\")\n",
    "axes[0].set_xlabel(\"Hour of Day\")\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "axes[0].set_xticks(range(24))\n",
    "\n",
    "# Plot histogram for fraudulent transactions\n",
    "sns.histplot(x='hour', data=df[df[\"is_fraud\"] == 1],\n",
    "             stat=\"density\", bins=24, ax=axes[1], color=\"green\")\n",
    "axes[1].set_title(\"Fraud\")\n",
    "axes[1].set_xlabel(\"Hour of Day\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].set_xticks(range(24))\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8984e",
   "metadata": {},
   "source": [
    "### Transaction Amount by category and Fraud Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Txn amt by cat and fraud\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x = \"category\", y = \"amt\", data = df, hue = \"is_fraud\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Transaction Amount by Category and Fraud Status')\n",
    "plt.xlabel('Transaction Category')\n",
    "plt.ylabel('Transaction Amount ($)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df2c06",
   "metadata": {},
   "source": [
    "### Counts of Fraudulent Transactions by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_counts = df.groupby('category')['is_fraud'].sum()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "fraud_counts.plot(kind='bar', color='skyblue')\n",
    "\n",
    "plt.xlabel('Transaction Category')\n",
    "plt.ylabel('Count of Fraudulent Transactions')\n",
    "plt.title('Counts of Fraudulent Transactions by Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4c772",
   "metadata": {},
   "source": [
    "#### Checking outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['amt', 'lat', 'long', 'city_pop']  \n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(columns), figsize=(6*len(columns), 6))\n",
    "for i, col in enumerate(columns):\n",
    "    sns.boxplot(x=df[col], ax=axes[i], orient='h', palette='Set2')  \n",
    "    axes[i].set_title(f'Box Plot of {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080025fa",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Age at Transactions\n",
    "df['dob'] = pd.to_datetime(df['dob']) \n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time']) \n",
    "df['age_at_txns'] = (df['trans_date_trans_time'] - df['dob']).dt.days // 365 \n",
    "\n",
    "# 2. Time Since Last Transactions\n",
    "df.sort_values(['cc_num', 'trans_date_trans_time'], inplace=True) \n",
    "df['time_since_last_txn'] = df.groupby('cc_num')['trans_date_trans_time'].diff().dt.days  \n",
    "\n",
    "# 3. Transaction Amount Relative to Average\n",
    "df['avg_txn_amount'] = df.groupby('cc_num')['amt'].transform('mean')  \n",
    "df['txn_amount_relative_to_avg'] = df['amt'] / df['avg_txn_amount']  \n",
    "\n",
    "# 4. Cumulative Transactions Amount\n",
    "df['cumulative_txn_amount'] = df.groupby('cc_num')['amt'].cumsum()  \n",
    "\n",
    "\n",
    "print(df[['age_at_txns', 'time_since_last_txn', 'txn_amount_relative_to_avg', 'cumulative_txn_amount']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fafc2e",
   "metadata": {},
   "source": [
    "### Dropping some columns and data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Unnamed' and cc_num column\n",
    "df.drop(columns=['Unnamed: 0','cc_num'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['first', 'unix_time', 'dob', 'zip', 'city','street', 'state', 'trans_num', 'trans_date_trans_time','last'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['merchant'] = df['merchant'].apply(lambda x : x.replace('fraud_',''))\n",
    "df['gender'] = df['gender'].map({'F': 0, 'M': 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c97113",
   "metadata": {},
   "source": [
    "#### Converting categorical columns into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35671a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_column = ['job', 'merchant', 'category', 'lat']\n",
    "woe_encoder = WOEEncoder()\n",
    "\n",
    "df_trasform = woe_encoder.fit_transform(df[encode_column], df['is_fraud'])\n",
    "\n",
    "df[encode_column] = df_trasform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1966b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trasform.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82352bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns \n",
    "df.drop(columns=['hour','age_at_txns', 'long', 'city_pop', 'merchant','lat','time_since_last_txn', 'cumulative_txn_amount', 'txn_amount_relative_to_avg', 'time_since_last_txn'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57474f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Check the scaled data\n",
    "print(df_scaled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb393bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for the entire DataFrame\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "plt.title(\"Correlation Matrix of All Columns\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187cc7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(labels='is_fraud', axis=1) \n",
    "y = df.loc[:,'is_fraud']               \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a905da1",
   "metadata": {},
   "source": [
    "#### Balancing dataset using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3469c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=1)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_train_resampled.value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Class in Resampled Training Data')\n",
    "plt.axis('equal')  \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c476c64-f508-4174-a802-1d3def2c4767",
   "metadata": {},
   "source": [
    "Imbalanced classification modelling with synthetic oversampling and stratified K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b09d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd01c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "\n",
    "# List to store metrics\n",
    "roc_auc_scores, fprs, tprs, precisions, recalls, f1_scores = [], [], [], [], [], []\n",
    "confusion_matrices, accuracies = [], []\n",
    "\n",
    "# Stratified cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in skf.split(X_train_resampled, y_train_resampled):\n",
    "    X_train_fold, X_test_fold = X_train_resampled.iloc[train_index], X_train_resampled.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train_resampled.iloc[train_index], y_train_resampled.iloc[test_index]\n",
    "\n",
    "    # Fit and predict\n",
    "    classifier.fit(X_train_fold, y_train_fold)\n",
    "    y_scores = classifier.predict_proba(X_test_fold)[:, 1]\n",
    "    y_pred = (y_scores > 0.5)\n",
    "\n",
    "    # Calculate metrics\n",
    "    fpr, tpr, _ = roc_curve(y_test_fold, y_scores)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    roc_auc_scores.append(auc(fpr, tpr))\n",
    "    precision, recall, _ = precision_recall_curve(y_test_fold, y_scores)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    conf_mat = confusion_matrix(y_test_fold, y_pred)\n",
    "    confusion_matrices.append(conf_mat)\n",
    "    accuracies.append(accuracy_score(y_test_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_test_fold, y_pred))\n",
    "\n",
    "# Results\n",
    "print(f'Average Accuracy for Random Forest: {np.mean(accuracies):.2f}')\n",
    "print(f'Average F1 Score for Random Forest: {np.mean(f1_scores):.2f}')\n",
    "print(f'Average ROC AUC Score for Random Forest: {np.mean(roc_auc_scores):.2f}')\n",
    "print(f'Average Precision for Random Forest: {np.mean([np.mean(precision) for precision in precisions]):.2f}')\n",
    "print(f'Average Recall for Random Forest: {np.mean([np.mean(recall) for recall in recalls]):.2f}')\n",
    "\n",
    "def plot_roc_curves(fprs, tprs, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(len(fprs)):\n",
    "        plt.plot(fprs[i], tprs[i], lw=1, label=f'Fold {i+1}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves for {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curves(precisions, recalls, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(len(precisions)):\n",
    "        plt.plot(recalls[i], precisions[i], lw=1, label=f'Fold {i+1}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curves for {model_name}')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix.astype(int), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "# Visualisation\n",
    "plot_roc_curves(fprs, tprs, \"Random Forest\")\n",
    "plot_precision_recall_curves(precisions, recalls, \"Random Forest\")\n",
    "plot_confusion_matrix(np.mean(confusion_matrices, axis=0), \"Random Forest\")\n",
    "\n",
    "# Feature Importance\n",
    "feature_importances = pd.Series(classifier.feature_importances_, index=X_train_resampled.columns)\n",
    "plt.figure(figsize=(8, 6))\n",
    "feature_importances.sort_values().plot(kind='barh')\n",
    "plt.title('Feature Importance - Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd5aae",
   "metadata": {},
   "source": [
    "### Decision Tree Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fd25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# List to store metrics\n",
    "roc_auc_scores, fprs, tprs, precisions, recalls, f1_scores = [], [], [], [], [], []\n",
    "confusion_matrices, accuracies = [], []\n",
    "\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in skf.split(X_train_resampled, y_train_resampled):\n",
    "    X_train_fold, X_test_fold = X_train_resampled.iloc[train_index], X_train_resampled.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train_resampled.iloc[train_index], y_train_resampled.iloc[test_index]\n",
    "\n",
    "    # Fit and predict\n",
    "    classifier.fit(X_train_fold, y_train_fold)\n",
    "    y_scores = classifier.predict_proba(X_test_fold)[:, 1]\n",
    "    y_pred = (y_scores > 0.5)\n",
    "\n",
    "    # Calculate metrics\n",
    "    fpr, tpr, _ = roc_curve(y_test_fold, y_scores)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    roc_auc_scores.append(auc(fpr, tpr))\n",
    "    precision, recall, _ = precision_recall_curve(y_test_fold, y_scores)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    conf_mat = confusion_matrix(y_test_fold, y_pred)\n",
    "    confusion_matrices.append(conf_mat)\n",
    "    accuracies.append(accuracy_score(y_test_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_test_fold, y_pred))\n",
    "\n",
    "#results\n",
    "print(f'Average Accuracy for Decision Tree: {np.mean(accuracies):.2f}')\n",
    "print(f'Average F1 Score for Decision Tree: {np.mean(f1_scores):.2f}')\n",
    "print(f'Average ROC AUC Score for Decision Tree: {np.mean(roc_auc_scores):.2f}')\n",
    "print(f'Average Precision for Decision Tree: {np.mean([np.mean(precision) for precision in precisions]):.2f}')\n",
    "print(f'Average Recall for Decision Tree: {np.mean([np.mean(recall) for recall in recalls]):.2f}')\n",
    "\n",
    "def plot_roc_curves(fprs, tprs, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(len(fprs)):\n",
    "        plt.plot(fprs[i], tprs[i], lw=1, label=f'Fold {i+1}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves for {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curves(precisions, recalls, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(len(precisions)):\n",
    "        plt.plot(recalls[i], precisions[i], lw=1, label=f'Fold {i+1}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curves for {model_name}')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix.astype(int), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "# Visualisations\n",
    "plot_roc_curves(fprs, tprs, \"Decision Tree\")\n",
    "plot_precision_recall_curves(precisions, recalls, \"Decision Tree\")\n",
    "plot_confusion_matrix(np.mean(confusion_matrices, axis=0), \"Decision Tree\")\n",
    "\n",
    "# Feature importance visualization\n",
    "feature_importances = pd.Series(classifier.feature_importances_, index=X_train_resampled.columns)\n",
    "plt.figure(figsize=(8, 6))\n",
    "feature_importances.sort_values().plot(kind='barh')\n",
    "plt.title('Feature Importance - Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3397a",
   "metadata": {},
   "source": [
    "### KNN Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "# Store metrics\n",
    "roc_auc_scores, fprs, tprs, precisions, recalls, f1_scores = [], [], [], [], [], []\n",
    "confusion_matrices, accuracies = [], []\n",
    "\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in skf.split(X_train_resampled, y_train_resampled):\n",
    "    X_train_fold, X_test_fold = X_train_resampled.iloc[train_index], X_train_resampled.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train_resampled.iloc[train_index], y_train_resampled.iloc[test_index]\n",
    "\n",
    "    # Fit and predict\n",
    "    classifier.fit(X_train_fold, y_train_fold)\n",
    "    y_scores = classifier.predict_proba(X_test_fold)[:, 1]\n",
    "    y_pred = (y_scores > 0.5)\n",
    "\n",
    "    # Collect and calculate metrics\n",
    "    fpr, tpr, _ = roc_curve(y_test_fold, y_scores)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    roc_auc_scores.append(auc(fpr, tpr))\n",
    "    precision, recall, _ = precision_recall_curve(y_test_fold, y_scores)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    conf_mat = confusion_matrix(y_test_fold, y_pred)\n",
    "    confusion_matrices.append(conf_mat)\n",
    "    accuracies.append(accuracy_score(y_test_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_test_fold, y_pred))\n",
    "\n",
    "# Results\n",
    "print(f'Average Accuracy for KNN: {np.mean(accuracies):.2f}')\n",
    "print(f'Average F1 Score for KNN: {np.mean(f1_scores):.2f}')\n",
    "print(f'Average ROC AUC Score for KNN: {np.mean(roc_auc_scores):.2f}')\n",
    "print(f'Average Precision for KNN: {np.mean([np.mean(precision) for precision in precisions]):.2f}')\n",
    "print(f'Average Recall for KNN: {np.mean([np.mean(recall) for recall in recalls]):.2f}')\n",
    "\n",
    "def plot_roc_curves(fprs, tprs, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(len(fprs)):\n",
    "        plt.plot(fprs[i], tprs[i], lw=1, label=f'Fold {i+1}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves for {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curves(precisions, recalls, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(len(precisions)):\n",
    "        plt.plot(recalls[i], precisions[i], lw=1, label=f'Fold {i+1}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curves for {model_name}')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix.astype(int), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "# Visualisations\n",
    "plot_roc_curves(fprs, tprs, \"KNN\")\n",
    "plot_precision_recall_curves(precisions, recalls, \"KNN\")\n",
    "plot_confusion_matrix(np.mean(confusion_matrices, axis=0), \"KNN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a2816",
   "metadata": {},
   "source": [
    "### Dynamic Synthetic Oversampling with Stratified K-Fold Cross-Validation within a Pipeline Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94553393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=1)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Prepare cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "cumulative_conf_matrix = np.zeros((2, 2))  # Assuming binary classification\n",
    "\n",
    "fig, (ax_roc, ax_pr) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Fiting the model\n",
    "    pipeline.fit(X_train_fold, y_train_fold)\n",
    "    y_pred = pipeline.predict(X_test_fold)\n",
    "    y_probs = pipeline.predict_proba(X_test_fold)[:, 1]\n",
    "\n",
    "    # Metrics collection\n",
    "    scores.append({\n",
    "        'roc_auc': roc_auc_score(y_test_fold, y_probs),\n",
    "        'f1': f1_score(y_test_fold, y_pred),\n",
    "        'accuracy': accuracy_score(y_test_fold, y_pred),\n",
    "        'recall': recall_score(y_test_fold, y_pred),\n",
    "        'precision': precision_score(y_test_fold, y_pred)\n",
    "    })\n",
    "    \n",
    "    # confusion matrix\n",
    "    cumulative_conf_matrix += confusion_matrix(y_test_fold, y_pred)\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test_fold, y_probs)\n",
    "    ax_roc.plot(fpr, tpr, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test_fold, y_probs)\n",
    "    ax_pr.plot(recall, precision, alpha=0.3)\n",
    "\n",
    "# Average metrics and ploting ROC & Precision-Recall\n",
    "ax_roc.plot([0, 1], [0, 1], 'k--')\n",
    "ax_roc.set_title('ROC Curve')\n",
    "ax_roc.set_xlabel('False Positive Rate')\n",
    "ax_roc.set_ylabel('True Positive Rate')\n",
    "\n",
    "ax_pr.set_title('Precision-Recall Curve')\n",
    "ax_pr.set_xlabel('Recall')\n",
    "ax_pr.set_ylabel('Precision')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate average metrics\n",
    "average_scores = {metric: np.mean([score[metric] for score in scores]) for metric in scores[0]}\n",
    "print(\"Average Metrics:\", average_scores)\n",
    "\n",
    "# display the cumulative confusion matrix\n",
    "print(\"Cumulative Confusion Matrix:\\n\", cumulative_conf_matrix.astype(int))\n",
    "\n",
    "# Defining the plot function for cumulative confusion matrix\n",
    "def plot_cumulative_confusion_matrix(matrix):\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(matrix, cmap='Blues')\n",
    "    plt.title('Cumulative Confusion Matrix')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_xticklabels([''] + ['Non-fraud', 'Fraud'])\n",
    "    ax.set_yticklabels([''] + ['Non-fraud', 'Fraud'])\n",
    "\n",
    "    # Looping over data dimensions and create text annotations.\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            ax.text(j, i, str(matrix[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the plot function\n",
    "plot_cumulative_confusion_matrix(cumulative_conf_matrix)\n",
    "\n",
    "# Feature Importance Visualisation\n",
    "feature_importances = pd.Series(pipeline.named_steps['classifier'].feature_importances_, index=X_train.columns)\n",
    "plt.figure(figsize=(10, 8))\n",
    "feature_importances.sort_values().plot(kind='barh')\n",
    "plt.title('Feature Importance - RandomForest')\n",
    "plt.show()\n",
    "\n",
    "# DataFrame for the metrics\n",
    "metrics_df = pd.DataFrame(scores)\n",
    "print(metrics_df.describe().transpose()[['mean', 'std']])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f0e97",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=1)),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "cumulative_conf_matrix = np.zeros((2, 2))  \n",
    "\n",
    "fig, (ax_roc, ax_pr) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Fitting the model\n",
    "    pipeline.fit(X_train_fold, y_train_fold)\n",
    "    y_pred = pipeline.predict(X_test_fold)\n",
    "    y_probs = pipeline.predict_proba(X_test_fold)[:, 1]\n",
    "\n",
    "    # Metrics collection\n",
    "    scores.append({\n",
    "        'roc_auc': roc_auc_score(y_test_fold, y_probs),\n",
    "        'f1': f1_score(y_test_fold, y_pred),\n",
    "        'accuracy': accuracy_score(y_test_fold, y_pred),\n",
    "        'recall': recall_score(y_test_fold, y_pred),\n",
    "        'precision': precision_score(y_test_fold, y_pred)\n",
    "    })\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cumulative_conf_matrix += confusion_matrix(y_test_fold, y_pred).astype(int)\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test_fold, y_probs)\n",
    "    ax_roc.plot(fpr, tpr, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test_fold, y_probs)\n",
    "    ax_pr.plot(recall, precision, alpha=0.3)\n",
    "\n",
    "# Finalize ROC & Precision-Recall plots\n",
    "ax_roc.plot([0, 1], [0, 1], 'k--')\n",
    "ax_roc.set_title('ROC Curve')\n",
    "ax_roc.set_xlabel('False Positive Rate')\n",
    "ax_roc.set_ylabel('True Positive Rate')\n",
    "\n",
    "ax_pr.set_title('Precision-Recall Curve')\n",
    "ax_pr.set_xlabel('Recall')\n",
    "ax_pr.set_ylabel('Precision')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate average metrics\n",
    "average_scores = {metric: np.mean([score[metric] for score in scores]) for metric in scores[0]}\n",
    "print(\"Average Metrics for Decision Tree:\", average_scores)\n",
    "\n",
    "\n",
    "def plot_cumulative_confusion_matrix(matrix):\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(matrix, cmap='Blues')\n",
    "    plt.title('Cumulative Confusion Matrix')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_xticks([0, 1, 2])  # Set the ticks first\n",
    "    ax.set_xticklabels(['', 'Non-fraud', 'Fraud'])  \n",
    "    ax.set_yticks([0, 1, 2])  # Set the ticks first\n",
    "    ax.set_yticklabels(['', 'Non-fraud', 'Fraud'])  \n",
    "\n",
    "    # Looping over data dimensions and create text annotations.\n",
    "    for i in range(matrix.shape[0]):  # Corrected indentation\n",
    "        for j in range(matrix.shape[1]):  # Corrected indentation\n",
    "            ax.text(j, i, str(matrix[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_cumulative_confusion_matrix(cumulative_conf_matrix)\n",
    "\n",
    "# Feature importance visualization\n",
    "feature_importances = pd.Series(pipeline.named_steps['classifier'].feature_importances_, index=X_train.columns)\n",
    "plt.figure(figsize=(10, 8))\n",
    "feature_importances.sort_values().plot(kind='barh')\n",
    "plt.title('Feature Importance - Decision Tree')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe482a",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the pipeline with imblearn's Pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=1)),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Cross-validation setup\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "cumulative_conf_matrix = np.zeros((2, 2), dtype=int)\n",
    "\n",
    "fig, (ax_roc, ax_pr) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train_fold, y_train_fold)\n",
    "    y_pred = pipeline.predict(X_test_fold)\n",
    "    y_probs = pipeline.predict_proba(X_test_fold)[:, 1]\n",
    "\n",
    "    # Metrics collection\n",
    "    scores.append({\n",
    "        'roc_auc': roc_auc_score(y_test_fold, y_probs),\n",
    "        'f1': f1_score(y_test_fold, y_pred),\n",
    "        'accuracy': accuracy_score(y_test_fold, y_pred),\n",
    "        'recall': recall_score(y_test_fold, y_pred),\n",
    "        'precision': precision_score(y_test_fold, y_pred)\n",
    "    })\n",
    "    \n",
    "    # Confusion matrix update\n",
    "    cumulative_conf_matrix += confusion_matrix(y_test_fold, y_pred)\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test_fold, y_probs)\n",
    "    ax_roc.plot(fpr, tpr, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test_fold, y_probs)\n",
    "    ax_pr.plot(recall, precision, alpha=0.3)\n",
    "\n",
    "# Finalizing ROC & Precision-Recall plots\n",
    "ax_roc.plot([0, 1], [0, 1], 'k--')\n",
    "ax_roc.set_title('ROC Curve')\n",
    "ax_roc.set_xlabel('False Positive Rate')\n",
    "ax_roc.set_ylabel('True Positive Rate')\n",
    "\n",
    "ax_pr.set_title('Precision-Recall Curve')\n",
    "ax_pr.set_xlabel('Recall')\n",
    "ax_pr.set_ylabel('Precision')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculating average metrics\n",
    "average_scores = {metric: np.mean([score[metric] for score in scores]) for metric in scores[0]}\n",
    "print(\"Average Metrics for KNN:\", average_scores)\n",
    "\n",
    "# Display cumulative confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cumulative_conf_matrix, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title(\"Cumulative Confusion Matrix - KNN\")\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3399829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions/probabilities) from model\n",
    "y_probs = pipeline.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.savefig(os.path.join(results_path, \"roc_curve.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73176d6-c739-404b-a9f9-920d8ed77214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
